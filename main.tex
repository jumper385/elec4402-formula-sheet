\documentclass[10pt]{article}

% Packages
\usepackage{graphicx} % For including images
\usepackage{amsmath}  % For math stuff...
\usepackage{amssymb}  % For additional math symbols
\usepackage{hyperref} % For hyperlinks
\usepackage{geometry} % For adjusting margins
\usepackage{fancyhdr} % For header and footer customization
\usepackage{lastpage} % For referencing the last page number
\usepackage{titlesec} % For access to section titles in the header
\geometry{margin=1in} % Set margin size

% Title
\title{ELEC4402 Formula Sheet}
\author{Henry Chen}
\date{\today}

\pagestyle{fancy}
\fancyhf{}                % Clear all header and footer fields

% Configure the header to display the section name on the left and title on the right
\fancyhead[L]{\leftmark}        % Left header: current section name
\fancyhead[R]{\thetitle}        % Right header: document title

% Configure the footer to display page number out of max page number
\fancyfoot[C]{Page \thepage\ of \pageref{LastPage}}

\renewcommand{\sectionmark}[1]{\markboth{#1}{}}

\begin{document}

% Title Page
\maketitle

% Abstract
\begin{abstract}
    Key formulae and helpers for ELEC4402 - Communication Systems. Hopefully this helps to carry!!!
\end{abstract}

% Table of Contents
\tableofcontents
\newpage

\section{Handy Formulae}
\subsection{Rectangle Functions}
\[
	\text{rect}(t) \rightarrow
	\begin{cases}
		1, & -\frac{1}{2} \leq t \leq \frac{1}{2} \\
		0, & \text{otherwise}
	\end{cases}
\]

\[
	\text{rect}\left(\frac{t}{T}\right) \rightarrow
	\begin{cases}
		1, & -\frac{T}{2} \leq t \leq \frac{T}{2} \\
		0, & \text{otherwise}
	\end{cases}
\]

\[
	\text{rect}\left(\frac{t - T}{T}\right) \rightarrow
	\begin{cases}
		1, & T - \frac{T}{2} \leq t \leq T + \frac{T}{2} \\
		0, & \text{otherwise}
	\end{cases}
\]

\subsection{Sinusoid Integration}
\[
	\int \sin(at) \, dt = -\frac{\cos(at)}{a} + C
\]
\[
	\int \cos(at) \, dt = \frac{\sin(at)}{a} + C
\]
\[
	\int e^{at} \, dt = \frac{e^{at}}{a} + C
\]

\section{Signal Characteristics}
\subsection{(Average) Power of a Signal}
$$
	P_x = \frac{1}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}} x^2(t) \, dt
$$

For sinusoidal signals, the power can be simplified to:
$$
	P_x = \frac{A_c^2}{2}
$$

\subsection{Energy of a Signal}
$$
	E = \int_{-\infty}^{\infty} x^2(t) \, dt
$$

\subsection{Representation of Narrowband (NB) Signals}
$$
	g(t) = a(t) \cos(\underbrace{2 \pi f_c t}_{\text{A}} + \underbrace{\phi(t)}_{\text{B}})
$$
Using the identity $\cos(A+B) = \cos(A)\cos(B) - \sin(A)\sin(B)$
\begin{align*}
	\tilde{g}(t) & = \underbrace{a(t) \cos(\phi(t))}_{\text{in-phase}} - \underbrace{a(t) \sin(\phi(t))}_{\text{quadrature}} \\
	g_i(t)       & = a(t) \cos(\omega_c t + \phi(t))                                                                         \\
	g_q(t)       & = a(t) \sin(\omega_c t + \phi(t))
\end{align*}

\subsection{Complex Envelope}
Given $g_i(t)$ and $g_q(t)$ and $a(t)$
\begin{align*}
	g(t) & = g(t) + j \cdot g(t) \\
	a(t) & = e^{j\phi(t)}
\end{align*}

\section{Amplitude Modulation (AM) Formulae}
\subsection{Conventional AM Signal}
\[
	s(t) = A_c\left[1 + \mu\cos(2 \pi f_m t)\right]\cos(2 \pi f_c t)
\]

\subsection{AM Modulation Index}

$$
	\mu = k_a \cdot A_m
$$

\noindent Where:
\begin{itemize}
	\item $0 \leq \mu \leq 1$
	\item $\mu = 1 \Rightarrow \text{overmodulated}$
\end{itemize}

\subsection{AM Power Efficiency}
\[
	\eta = \frac{\text{Baseband Power}}{\text{Total Power}} = \frac{\mu^2}{2 + \mu^2}
\]

\subsection{AM Signal Modulation Index}
\[
	\mu = \frac{A_{\text{max}} - A_{\text{min}}}{A_{\text{max}} + A_{\text{min}}}
\]
\noindent Where:
\begin{itemize}
	\item Maximum Amplitude: $A_{\text{max}} = A_c(1+\mu)$
	\item Minimum Amplitude: $A_{\text{max}} = A_c(1-\mu)$
\end{itemize}

\section{Frequency Modulation (FM) Formulae}

\subsection{FM Modulation Index}
\[
	\beta = \frac{\Delta f}{f_m} = \frac{k_f A_m}{f_m}
\]

\noindent Where:
\begin{itemize}
	\item Narrowband: \( \beta < 1 \)
	\item Wideband: \( \beta > 1 \)
\end{itemize}

\subsection{Maximum Frequency Deviation}
\begin{align*}
	\Delta f & = k_f \cdot A_m   \\
	         & = \beta \cdot f_m
\end{align*}

\noindent Where:
\begin{itemize}
	\item $k_f$: Frequency Sensitivity; typically the VCO Specification
	\item $A_m$: Message Amplitude; the message signal is usually injected into the VCO to drive the signal
	\item $\beta$: The FM Modulation Index
\end{itemize}

%TODO: Check this is correct...
\subsection{FM Complex Envelope}
\begin{align*}
	s(t) & = A_c \cos(2\pi f_c t + \beta \sin(2 \pi f_m t))                                 \\
	     & = Re\left[A_c \cdot e^{2\pi f_c tj + j\beta\sin(2\pi f_m t)}\right]              \\
	     & = Re\left[A_c \cdot e^{2\pi f_c tj} \cdot e^{j\beta\cdot\sin(2\pi f_m t)}\right] \\
	     & = Re\left[\tilde{s}(t)\cdot e^{j\beta\cdot\sin(2\pi f_c t)}\right]               \\
\end{align*}

\noindent Where:
\begin{itemize}
	\item $\tilde{s}(t) = A_c \cdot e^{j \beta\cdot\sin(2\pi f_m t))}$: The complex envelope of the FM signal
\end{itemize}

\subsection{Single Tone FM Signal}
\subsubsection{Time Domain Representation}
\begin{align*}
	s(t)         & = A_c \sum_{n=-\infty}^{\infty}  J_n(\beta)\cos\left[2 \pi (f_c + n f_m) t\right] \\
	\tilde{s}(t) & = A_c \sum_{n=-\infty}^{\infty} J_n(\beta) e^{2 \pi \cdot n f_m t \cdot j}        \\
\end{align*}
\subsubsection{Frequency Domain Representation}
\[
	s(f) = \frac{A_c}{2} \sum_{n=-\infty}^{\infty} \underbrace{J_n(\beta)\left[\delta(f-f_c-n f_m) + \delta(f+f_c+n f_m)\right]}_{\text{Bessel Function of Delta Functions}}
\]

\subsection{Carson's Rule}
\subsubsection{Narrow Band Signals ($\beta < 1$)}
\[
	B = 2 f_m
\]

\subsubsection{Wideband Signals ($\beta > 1$)}
\[
	B = 2 \Delta f + 2 f_m
\]
\subsubsection{For Arbitrary Signals}
\begin{align*}
	W & = 2 D W + 2 W                                                                            \\
	W & = 2DW(1 + \frac{1}{D})                                                                   \\
	  & = 2 \cdot \underbrace{\max_{t}\left|k_f m(t)\right|}_{\text{$k_f$ x message bw}} + 2 f_m
\end{align*}

\noindent Where:
\begin{itemize}
	\item $D = \frac{\Delta f}{W} $: Frequency Deviation of the arbitrary signal
\end{itemize}

\subsection{Arbitrary Signal Helpers for FM}
\[
	f(t) = \frac{1}{2\pi} \cdot \frac{d\theta(t)}{d(t)} = f_c + k_f m(t)
\]

Generic form of $s(t)$:
\[
	s(t) = A_c\cos(\underbrace{2 \pi f_c t}_{\text{carrier}} + \underbrace{\beta m(t)}_{\text{baseband}})
\]

Base-Band Frequency of a Signal:
\[
	f(t) = f_c + k_f m(t)
\]

\section{Power Spectral Density (PSD)}
\[
	G(f) = \lim_{T \to \infty} \frac{|X_T(f)|^2}{T}
\]

\subsection{PSD through an LTI System}
\[
    G_y(f) = \left|H(f)\right|^2 \cdot G_x(f)
\]

\noindent Where:
\begin{itemize}
    \item $H(f)$: Fourier Transform of the LTI System's Transfer. By squaring it, you then get the Power Spectral Density
\end{itemize}

\subsection{PSD of a Random Process}
\[
G_X(f) = \lim_{T \to \infty} \frac{\mathbb{E} \left\{ |X_T(f, \varepsilon_i)|^2 \right\}}{T}
\]

\noindent\textbf{NOTE:} very similar for deterministic signals PSD

\noindent\textbf{NOTE:} The PSD is an ensemble average of the powers of the of captured samples

\subsection{Power of a PSD}
\[
    P = \int_{-\infty}^{\infty} G_X(f) \, d\!f 
\]

\subsection{PSD Of Narrowband Random Processes}
The PSD of random processes (noise) are the same in both, the in-phase and quadrature components (i.e. $G_{x_s} == G_{x_c}$).

\[
    x(t) = x_c \cos(2\pi f_c t) - x_s \sin(2 \pi f_c t)
\]

\noindent Where:
\begin{itemize}
    \item $x(t)$: Is a random process
    \item $x_c\cos(2\pi f_c t)$: In-Phase component of $x(t)$.
    \item $x_s\sin(2\pi f_c t)$: Quadratur component of $x(t)$.
\end{itemize}

In this case, the upconverted PSD of the signals are also equal within the bandwidth of the upconverted channel.
\[
    G_{x_s} = G_{x_c} =
    \begin{cases}
      G_x(f + f_c) + G_x(f - f_c) & \text{if } |f| < B\\
      0 & \text{otherwise}
   \end{cases}
\]

\section{Sampling and Quantization}
\subsection{Nyquist Rate}
The Nyquist Rate determines the sample frequency the receiver must sample the basebaind signal $m(t)$ at. This value is twice the bandwidth: 
\[
    R_{\text{sample}} = 2 \times \text{Signal Bandwidth}
\]
\subsection{Pulse Train Fourier Transform}
The fourier transform of a periodid pulse train in the tiem domain leads to a series of impulses in the frequency domain:
\[
    \sum_{m=-\infty}^{\infty} \delta(t - m T_0) \xrightarrow{\mathcal{F}T} \sum_{n=-\infty}^{\infty} \delta(f - n f_0)
\]
\noindent Where:
\begin{itemize}
    \item $f_0=1/T_0$: is the sampling frequency of the pulse train
\end{itemize}

\noindent $\therefore$ the Fourier Transform of a periodic pulse train results in a series of delta functions, representing the discrete frequency components present in the signal. The spectrm of $x(f)$ - the signal being sampled - is replicated around mulitples of $f_0$.

\subsection{Quantizer Step Size}
\[
    \Delta = \frac{2V}{N} = \frac{2V}{2^m}
\]
\noindent Where:
\begin{itemize}
    \item $V$: The amplitude of the signal
    \item $N$: Number of Quantization Steps
    \item $m$: Bit depth of quantization ($m=\log_2{N}$)
\end{itemize}

\section{Pulse Amplitude Modulation (PAM)}

\begin{itemize}
    \item \( T \): Symbol period (duration of each symbol).
    \item \( D = \frac{1}{T} \): Symbol rate or pulse rate.

    \item Note: A single symbol can represent multiple bits, depending on the modulation scheme.
\end{itemize}

\section{PAM Signal Representation}
The PAM signal for the \( m \)-th symbol, \( s_m(t) \), is given by:
\[
s_m(t) = A_m \cdot V(t)
\]
where:
\begin{itemize}
    \item \( A_m \): Amplitude corresponding to the \( m \)-th symbol.
    \item \( V(t) \): Fixed pulse shape.
    \item \( m = 1, 2, \dots, M \), where \( M \) is the number of symbols, typically based on the quantization levels.
\end{itemize}

\subsection{Bit Duration and Bit Rate}
\begin{itemize}
    \item \( T_b \): Bit duration, or time taken to transmit one bit.
    \item \( R_b \): Bit rate, or number of bits transmitted per second $1/T_b$.
\end{itemize}

\subsection{Relationship between Bit Rate, Symbol Rate, and Bits per Symbol}
The relationship between bit duration, bit rate, symbol rate, and the number of bits per symbol is given by:
\[
T_b = \frac{1}{R_b} = \frac{1}{kD} = \frac{T}{k} = \frac{T}{\log_2 M}
\]
where:
\begin{itemize}
    \item \( k \): Number of bits per symbol.
    \item \( M \): Number of distinct symbols in the signal set, with \( M = 2^k \), so that each symbol represents \( k = \log_2 M \) bits.
\end{itemize}

\section{Spectra of Linearly Modulated Digital Signals}

\subsection{Signal Model}
The signal \( s(t) \) for a linearly modulated digital signal is given by:
\[
s(t) = \sum_{n=-\infty}^{\infty} A_n V(t + \Delta - nT)
\]
where:
\begin{itemize}
    \item \( A_n \): Sequence of scalar symbols (data).
    \item \( V(t) \): Pulse shape (basis function).
    \item \( T \): Symbol period.
    \item \( \Delta \): Random channel delay, uniformly distributed over \( [0, T] \).
\end{itemize}

\subsection{Power Spectral Density (PSD)}
The Power Spectral Density \( G(f) \) of \( s(t) \) is:
\[
G(f) = \frac{|V(f)|^2}{T} \sum_{\ell=-\infty}^{\infty} R(\ell) e^{-j 2 \pi f T \ell}
\]
where:
\begin{itemize}
    \item \( |V(f)|^2 \): Magnitude squared of the Fourier Transform of \( V(t) \).
    \item \( R(\ell) \): Autocorrelation function of the sequence \( \{A_n\} \) at lag \( \ell \).
    \item \( e^{-j 2 \pi f T \ell} \): Frequency shift term.
\end{itemize}

\subsection{Frequency Domain Summation of Delta Functions}
The periodic nature of the spectrum due to sampling can be represented as:
\[
\sum_{\ell=-\infty}^{\infty} e^{-j 2 \pi f T \ell} = \frac{1}{T} \sum_{k=-\infty}^{\infty} \delta \left(f - \frac{k}{T}\right)
\]
where:
\begin{itemize}
    \item Each delta function at \( f = \frac{k}{T} \) represents an impulse in the spectrum, showing periodic repetition at multiples of the symbol rate \( \frac{1}{T} \).
\end{itemize}

\section{Symbol Distance and Basis Functions}
Basis functions play a critical role in transforming vector spaces into a form that facilitates simpler calculations, especially for distance metrics. By mapping an $n$-dimensional vector space into an $n$-dimensional Euclidean space, each axis becomes orthogonal and has unit length. This transformation provides a clear framework for calculating distances and simplifies operations on vectors.

\subsection{Mathematical Representation}

Let a vector be represented in terms of basis functions as follows:
\[
\mathbf{v} = A \phi_1 + B \phi_2 + \ldots + Z \phi_m
\]
where:
\begin{itemize}
    \item \( \phi_i \) represents the basis functions.
    \item \( A, B, \ldots, Z \) are scalar multiples corresponding to each basis function \( \phi_i \).
\end{itemize}

\section{Distance Calculation}
In this representation, the Euclidean distance (or the magnitude of the vector \( \mathbf{v} \)) can be computed as:
\[
d = \sqrt{(A \phi_1)^2 + (B \phi_2)^2 + \ldots + (Z \phi_m)^2}
\]
This formula leverages the orthogonality of the basis functions, simplifying the computation of distances in the Euclidean space.

\subsection{Benefits of Euclidean Transformation}
Mapping a vector to its Euclidean representation using orthogonal basis functions offers several advantages:
\begin{itemize}
    \item \textbf{Simplified Distance Computation}: In a Euclidean space, the distance between points is computed using the Pythagorean theorem, which is easier and more intuitive.
    \item \textbf{Ease of Representation}: Each vector component aligns with an orthogonal axis, allowing for straightforward interpretation and manipulation of vectors in terms of individual components.
    \item \textbf{Application in Signal Processing}: Basis functions are often used in signal processing to break down complex signals into simpler components, facilitating analysis and processing.
\end{itemize}

In summary, basis functions enable the transformation of complex spaces into manageable Euclidean forms, making calculations like distances more intuitive and easier to compute.


\section{Matched Filter Overview}
A matched filter is designed to maximize the signal-to-noise ratio (SNR) at the output for a known signal in the presence of additive white Gaussian noise (AWGN). Given a signal \( s(t) \), the matched filter \( h(t) \) is defined as:
\[
h(t) = s(T - t)
\]
where \( T \) is the duration of the signal. This filter aligns with the signal \( s(t) \) and maximizes its correlation with the received signal \( r(t) = s(t) + n(t) \), thereby improving detection performance in noisy conditions.

\subsection{Deriving the Matched Filter}
The process of deriving the matched filter involves the following steps:
\begin{enumerate}
    \item \textbf{Maximizing SNR}: To maximize the output SNR, we choose a filter \( h(t) \) that aligns with the signal \( s(t) \).
    \item \textbf{Impulse Response}: The impulse response \( h(t) \) that achieves this SNR maximization is \( h(t) = s(T - t) \), which is the time-reversed version of the signal.
    \item \textbf{Convolution with Received Signal}: Convolve the received signal \( r(t) = s(t) + n(t) \) with \( h(t) \):
    \[
    y(t) = r(t) * h(t) = \int_{-\infty}^{\infty} r(\tau) h(t - \tau) \, d\tau
    \]
    \item \textbf{Decision Based on Output Peak}: Sample the output \( y(t) \) at \( t = T \). A high output value at \( t = T \) indicates the presence of \( s(t) \) in the received signal.
\end{enumerate}

\subsection{Differential Matched Filter for Binary Detection}
A differential matched filter is commonly used for binary detection (i.e., \( M = 2 \)), where there are only two possible signals, \( s_1(t) \) and \( s_2(t) \). Instead of constructing separate matched filters for each signal, a single differential matched filter can be used, defined as:
\[
h(t) = s_1(T - t) - s_2(T - t)
\]
The differential matched filter is derived based on the following principles:
\begin{enumerate}
    \item \textbf{Difference Between Signals}: By taking the difference between \( s_1(t) \) and \( s_2(t) \), the filter is tuned to maximize the difference in response for the two symbols.
    \item \textbf{Convolution and Decision Rule}: The received signal \( r(t) \) (which is either \( s_1(t) + n(t) \) or \( s_2(t) + n(t) \)) is convolved with \( h(t) \):
    \[
    y(t) = r(t) * h(t)
    \]
    This convolution produces a positive output if \( r(t) \approx s_1(t) \) and a negative output if \( r(t) \approx s_2(t) \). The detection decision can be made by simply checking the sign of \( y(T) \):
    \[
    \text{Detected symbol} =
    \begin{cases}
        s_1, & \text{if } y(T) > 0 \\
        s_2, & \text{if } y(T) < 0
    \end{cases}
    \]
\end{enumerate}

\section{Matched Filter for Multiple Symbols (M-ary Detection)}
For M-ary detection (where \( M > 2 \)), the differential matched filter approach does not generalize effectively. Instead, a separate matched filter is constructed for each symbol \( s_k(t) \) as follows:
\[
h_k(t) = s_k(T - t)
\]
To detect which symbol was transmitted:
\begin{enumerate}
    \item \textbf{Convolution for Each Symbol}: Convolve the received signal \( r(t) \) with each matched filter \( h_k(t) \), resulting in outputs \( y_k(T) \) for each filter.
    \item \textbf{Decision Rule}: The transmitted symbol is determined by selecting the filter with the highest output:
    \[
    \hat{k} = \arg \max_{k} y_k(T)
    \]
\end{enumerate}

\subsubsection{Why Differential Matched Filters Do Not Generalize for \( M > 2 \)}
For binary detection (\( M = 2 \)), a differential matched filter \( h(t) = s_1(T - t) - s_2(T - t) \) works well because there are only two hypotheses, and the output sign can be used to distinguish between the two symbols. However, this approach does not generalize for \( M > 2 \) because:
\begin{itemize}
    \item A single differential filter cannot uniquely represent multiple symbols.
    \item Each symbol requires a distinct matched filter for reliable identification in M-ary systems.
\end{itemize}

\subsection{Optimality of Matched Filters}
The matched filter is optimal in Gaussian noise as it maximizes the SNR at the output, achieving maximum likelihood detection. This makes matched filtering an effective tool for symbol detection in noisy environments.

\section{Bit Error Rate}
\[
    \text{BER} = Q\left(\sqrt{\frac{d^2}{2N_o}}\right)
\]
\begin{itemize}
    \item $d$: The average distance between symbols. 
    \item $N_o$: Noise Figure
\end{itemize}
\subsection{Calulating BER or Matched Filters}
To calcualte the bit error rate of matched filters, you will need to derive $d^2$ from scratch. This is where the following comes into play:
\[
    d^2 = \int_{-\infty}^{\infty}\underbrace{\left|{s_1(t) - s_2(t)}_\right|^2}_{|\text{matched filter}|^2}\,dt
\]
In this case, the piecewise function of the matched filter $h(t)$ can be substituted into the function. 

To calculate the BER, make sure to also have a target bit rate $R_b$ usually stated in the question/problem. Remember that $T_b = 1/R_b$

\section{Information Theory}
\subsection{Entropy}
\[
    H(p) = - \sum_{i=1}^{N} p_i \log_2(p_i)
\]

\subsection{Joint Entropy}
\[
	H(X,Y) = - \sum \sum p(x,y) \log_2 p(x,y)
\]

\subsection{Weakly Symmetric Channel Capacity}

The channel capacity \( C \) for a weakly symmetric channel can be defined as:

\[
    C = \log_2(N) - H(p)
\]

\noindent where:

\begin{itemize}
    \item \( N \): The number of symbols that the channel can output.
    \item \( p \): Probability vector representing the probability distribution of symbols, which is used to calculate the symbol entropy \( H(p) \).
\end{itemize}

The term \( H(p) \) denotes the entropy of the probability distribution \( p \), and it can be expanded as follows:
\[
    H(p) = - \sum_{i=1}^{N} p_i \log_2(p_i)
\]
where $p_i$ represents the probability of each symbol $i$ in the output set. The capacity $C$ represents the maximum achievable rate of information transfer through the channel, measured in bits per symbol.

\subsection{Code Rate \( R_c \)}

The \textit{code rate} \( R_c \) is a measure of the efficiency of an error-correcting code. It represents the ratio of useful information to the total transmitted data, including the redundant bits added for error correction. Mathematically, the code rate \( R_c \) is defined as:

\[
    R_c = \frac{k}{n} = \frac{log_2{M}}{n}
\]

where:
\begin{itemize}
    \item \( k \): Number of information bits (useful data) per codeword.
    \item \( n \): Total number of bits per codeword, which includes both information bits and redundant bits for error correction.
    \item $m$: Number of unique symbols in the frame.
\end{itemize}

\noindent The code rate \( R_c \) ranges between 0 and 1:
\begin{itemize}
    \item When \( R_c = 1 \), all bits transmitted are information bits, with no redundancy, meaning no error correction is applied.
    \item When \( R_c \) is closer to 0, a larger proportion of bits are dedicated to error correction, increasing reliability but reducing the effective data rate.
\end{itemize}

\subsubsection{Example Calculation}

Consider a code with:
\begin{itemize}
    \item \( k = 4 \): 4 information bits per codeword
    \item \( n = 7 \): 7 total bits per codeword (4 information bits + 3 redundant bits)
\end{itemize}

Then the code rate \( R_c \) is calculated as:

\[
R_c = \frac{4}{7} \approx 0.57
\]

This means that 57\% of the transmitted bits are information, while the remaining 43\% are used for error correction.

\subsubsection{Trade-Off Between Rate and Reliability}

The code rate \( R_c \) is chosen based on channel conditions. In noisy channels, a lower code rate (more redundancy) is preferred to enhance error resilience. In clear channels, a higher code rate is typically chosen to maximize data throughput.

\end{document}
